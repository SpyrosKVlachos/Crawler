{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0619bc8",
   "metadata": {},
   "source": [
    "Βήμα 1: Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01151c1",
   "metadata": {},
   "source": [
    "Στο πρόγραμμα αυτό πραγματοποιηείται web crawler και συλλέγονται τίτλοι (στοιχεία h1 κώδικα html) και τα περιεχόμενα content μαζί με το url τους. Σε κάθε έγγραφο δίνεται μοναδικό αύξον id. Τα δεδομένα αυτά αποθηκεύονται στο output.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a64543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Γίνεται λήψη δεδομένων 1/30\n",
      "Γίνεται λήψη δεδομένων 2/30\n",
      "Γίνεται λήψη δεδομένων 3/30\n",
      "Γίνεται λήψη δεδομένων 4/30\n",
      "Γίνεται λήψη δεδομένων 5/30\n",
      "Γίνεται λήψη δεδομένων 6/30\n",
      "Γίνεται λήψη δεδομένων 7/30\n",
      "Γίνεται λήψη δεδομένων 8/30\n",
      "Γίνεται λήψη δεδομένων 9/30\n",
      "Γίνεται λήψη δεδομένων 10/30\n",
      "Γίνεται λήψη δεδομένων 11/30\n",
      "Γίνεται λήψη δεδομένων 12/30\n",
      "Γίνεται λήψη δεδομένων 13/30\n",
      "Γίνεται λήψη δεδομένων 14/30\n",
      "Γίνεται λήψη δεδομένων 15/30\n",
      "Γίνεται λήψη δεδομένων 16/30\n",
      "Γίνεται λήψη δεδομένων 17/30\n",
      "Γίνεται λήψη δεδομένων 18/30\n",
      "Γίνεται λήψη δεδομένων 19/30\n",
      "Γίνεται λήψη δεδομένων 20/30\n",
      "Γίνεται λήψη δεδομένων 21/30\n",
      "Γίνεται λήψη δεδομένων 22/30\n",
      "Γίνεται λήψη δεδομένων 23/30\n",
      "Γίνεται λήψη δεδομένων 24/30\n",
      "Γίνεται λήψη δεδομένων 25/30\n",
      "Γίνεται λήψη δεδομένων 26/30\n",
      "Γίνεται λήψη δεδομένων 27/30\n",
      "Γίνεται λήψη δεδομένων 28/30\n",
      "Γίνεται λήψη δεδομένων 29/30\n",
      "Γίνεται λήψη δεδομένων 30/30\n",
      "Τα δεδομένα αποθηκεύτηκαν στο output.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "def crawl_wikipedia(start_url, max_pages=30, output_file='output.json'):\n",
    "    visited_pages = []\n",
    "    pages_to_visit = [start_url]\n",
    "    page_id = 1\n",
    "\n",
    "    while pages_to_visit and len(visited_pages) < max_pages:\n",
    "        current_url = pages_to_visit.pop(0)\n",
    "\n",
    "        try:\n",
    "            print(f\"Γίνεται λήψη δεδομένων {len(visited_pages)+1}/{max_pages}\")\n",
    "            response = requests.get(current_url)\n",
    "            response.encode = 'utf-8'\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Σφάλμα HTTP: {response.status_code} στη σελίδα {current_url}\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            title = soup.find('h1').get_text()\n",
    "            \n",
    "            content_div = soup.find('div', class_='mw-parser-output')\n",
    "            if content_div:\n",
    "                content = content_div.text\n",
    "                content = re.sub(r\"\\[\\d+\\]\", \"\", content)\n",
    "                content = re.sub(r\"\\n+\", \"\\n\\n\", content)\n",
    "                content = re.sub(r\"\\s+\", \" \", content).strip()\n",
    "            else:\n",
    "                content = \"Κείμενο μη διαθέσιμο.\"\n",
    "            \n",
    "            visited_pages.append({\"id\": page_id,\"url\": current_url, \"title\": title, \"content\": content})\n",
    "            page_id += 1\n",
    "\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('/wiki/') and ':' not in href:\n",
    "                    full_url = f\"https://en.wikipedia.org{href}\"\n",
    "                    if full_url not in [page['url'] for page in visited_pages] and full_url not in pages_to_visit:\n",
    "                        pages_to_visit.append(full_url)\n",
    "            \n",
    "            time.sleep(1)\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Σφάλμα σύνδεσης στη σελίδα {current_url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Σφάλμα κατα την Επεξεργασία της σελίδας {current_url}: {e}\")\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(visited_pages, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print (f\"Τα δεδομένα αποθηκεύτηκαν στο {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "    crawl_wikipedia(start_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f079cb1",
   "metadata": {},
   "source": [
    "Ακολουθεί η λίστα με τις βιβλιοθήκες που χρησιμοποιήθηκαν στο πρόγραμμα.\n",
    "Το requests χρησιμοποιείται για την αποστολή http προς τις σελιδες (του wikipedia στην περιπτωση μας).\n",
    "Το BeautifulSoup χρησιμοποιείται για την ανάλυση του HTML κώδικα.\n",
    "Το json χρησιμοποιείται για την αποθήκευση δεδομένων σε αρχείο json.\n",
    "Τo time χρησιμοποιείται για να προσθέσει καθυστερήσεις μεταξύ των αιτημάτων για την αποφυγή \"μπλοκαρίσματος\".\n",
    "To re χρησιμοποιείται για την εφαρμογή κανονικών εκφράσεων (regex) στην επεξεργασία κειμένου.\n",
    "Η εισαγωγή των βιβλιοθηκών γίνεται στο πάνω μέρος του κώδικα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ceba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d786ab",
   "metadata": {},
   "source": [
    "Στην συνέχεια ακολουθεί η κύρια συνάρτηση του προγράμματος, η crawl_wikipedia.\n",
    "Η συνάρτηση αυτή περιλαμβάνει:\n",
    "start_url: Το αρχικό url με το οποίο θα ξεκινήσει το πρόγραμμα.\n",
    "max_pages: Το μέγιστο αριθμό σελίδων που θα προσπαθεί να επισκεφτεί το πρόγραμμα. Ως default τιμή έχουμε βάλει το 30.\n",
    "output_file: Το όνομα του αρχείου στο οποίο θα αποθηκευτούν τα δεδομένα του crawling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5591218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_wikipedia(start_url, max_pages=30, output_file='output.json'):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeac3a67",
   "metadata": {},
   "source": [
    "Στην κύρια συνάρτηση οι μεταβλητές που υπάρχουν είναι οι εξής:\n",
    "visited_pages: Περιέχει όλες τις σελίδες που έχει επισκεφτεί ο κώδικας.\n",
    "pages_to_visit: Είναι η ουρά με τις σελίδες που δεν έχει επισκεφτεί ακόμα ο κώδικας"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e93c5b",
   "metadata": {},
   "source": [
    "Ακολουθεί ο βρόγχος αναζήτησης σελίδων. Ο βρόγχος συνεχίζεται όσο α) υπάρχουν σελίδες που δεν έχουν επισκεφτεί και β) ο αριθμός των σελίδων που έχουν επισκεφτεί είναι μικρότερος απο το max_pages. Το pop(0) παίρνει την πρώτη σελίδα απο το pages_to_visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395c03a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "while pages_to_visit and len(visited_pages) < max_pages:\n",
    "        current_url = pages_to_visit.pop(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb7349d",
   "metadata": {},
   "source": [
    "Ακολουθεί η λήψη των δεδομένων της τρέχουσας σελίδας. \n",
    "Εκτελείται μέσω του requests.get() αίτημα HTML στην τρέχουσα σελίδα. Σε περίπτωση μη επιτυχής απάντησης (status code 200), εμφανίζεται μήνυμα σφάλματος και η διαδικασία συνεχίζεται με την επόμενη σελίδα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4563a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "            print(f\"Γίνεται λήψη δεδομένων {len(visited_pages)+1}/{max_pages}\")\n",
    "            response = requests.get(current_url)\n",
    "            response.encode = 'utf-8'\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Σφάλμα HTTP: {response.status_code} στη σελίδα {current_url}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd030264",
   "metadata": {},
   "source": [
    "Ακολουθεί η ανάλυση του HTML.\n",
    "Με την χρήση του BeautifulSoup αναλύεται ο κώδικας για να βρεθεί ο τίτλος (h1) και το περιεχόμενο (τα υπόλοιπα στοιχεία της σελίδας).\n",
    "Στην συνέχεια αφαιρούνται οι παραπομπές και τροποποιούνται οι νέες γραμμες και τα κενά σημεία που υπάρχουν στον κώδικα της σελίδας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b682144b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            title = soup.find('h1').get_text()\n",
    "            \n",
    "            content_div = soup.find('div', class_='mw-parser-output')\n",
    "            if content_div:\n",
    "                content = content_div.text\n",
    "                content = re.sub(r\"\\[\\d+\\]\", \"\", content)\n",
    "                content = re.sub(r\"\\n+\", \"\\n\\n\", content)\n",
    "                content = re.sub(r\"\\s+\", \" \", content).strip()\n",
    "            else:\n",
    "                content = \"Κείμενο μη διαθέσιμο.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49baa6",
   "metadata": {},
   "source": [
    "Ακολουθεί η μορφή της αποθήκευσης των δεδομένων.\n",
    "Το πρόγραμμα αποθηκεύει τα δεδομένα αυξάνοντας καθε φορα κατα 1 το id, το url της τρέχουσας ιστοσελίδας, τον τίτλο (το h1) και το περιεχόμενο (τα υπόλοιπα στοιχεία της σελίδας)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48037499",
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_pages.append({\"id\": page_id,\"url\": current_url, \"title\": title, \"content\": content})\n",
    "            page_id += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2868d7",
   "metadata": {},
   "source": [
    "Ακολουθεί η αναζήτηση των συνδέσμων.\n",
    "Στην συνάρτηση αυτή αναζητούνται όλα τα <a href> στοιχεία του html κώδικα της σελίδας.\n",
    "Αν ο σύνδεσμος αυτός είναι εσωτερικός, δηλαδή ξεκινάει με /wiki/ και δεν περιέχει \":\" γίνεται δημιουργία του πλήρες url για την σελίδα αυτή.\n",
    "Γίνεται επίσης έλεγχος ώστε α) να μην έχει γίνει ήδη \"επίσκεψη\" στην σελίδα και β) να μην βρίσκεται στην ουρά προς \"επίσκεψη\". Αν πληρούνται αυτά τα κριτήρια, τοτε το url προστίθεται στο pages_to_visit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb7baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('/wiki/') and ':' not in href:\n",
    "                    full_url = f\"https://en.wikipedia.org{href}\"\n",
    "                    if full_url not in [page['url'] for page in visited_pages] and full_url not in pages_to_visit:\n",
    "                        pages_to_visit.append(full_url)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce23a037",
   "metadata": {},
   "source": [
    "Ακολουθεί η καθυστέρηση.\n",
    "Η συνάρτηση αυτή υπάρχει ώστε να μην γίνει υπερφόρτωση του server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00321822",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d14da",
   "metadata": {},
   "source": [
    "Ακολουθούν οι εξαιρέσεις του κώδικα.\n",
    "Γίνεται εκτύπωση σχετικών μηνυμάτων σε σφάλματα σύνδεσης ή επεξεργασίας της σελίδας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2c17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "except requests.exceptions.RequestException as e:\n",
    "            print(f\"Σφάλμα σύνδεσης στη σελίδα {current_url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Σφάλμα κατα την Επεξεργασία της σελίδας {current_url}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb7703f",
   "metadata": {},
   "source": [
    "Ακολουθεί η αποθήκευση στο json αρχείο.\n",
    "Όταν οι παραπάνω διεργασίες ολοκληρωθούν, τα δεδομένα θα αποθηκευτούν στο output.json με την χρήση της συνάρτησης json.dump()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c5bac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        json.dump(visited_pages, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print (f\"Τα δεδομένα αποθηκεύτηκαν στο {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece454ea",
   "metadata": {},
   "source": [
    "Ακολουθεί η εκκίνηση του προγράμματος.\n",
    "Η μεταβλητή start_url περιέχει το url της ιστοσελίδας απο την οποία επιθυμούμε να ξεκινήσει το πρόγραμμα.\n",
    "Γίνεται κλήση της συνάρτησης crawl_wikipedia() με τo αρχικό url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf16d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "    crawl_wikipedia(start_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef05a935",
   "metadata": {},
   "source": [
    "Βήμα 2: Προ-επεξεργασία"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546f12c",
   "metadata": {},
   "source": [
    "Στον κώδικα αυτόν πραγματοποιείται προεπεξεργασία στο output.json και αποθηκέυει τα νέα δεδομένα στο processed_file.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f254e416",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\spyro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\spyro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\spyro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\spyro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import tee, islice, chain\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "lemmatizer_en = WordNetLemmatizer()\n",
    "\n",
    "stop_words_en = set(stopwords.words('english'))\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatize_text(tokens):\n",
    "    lemmatized_tokens = []\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    for token, tag in pos_tags:\n",
    "        if re.match(r'[a-zA-Z]+', token):\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemmatized_tokens.append(lemmatizer_en.lemmatize(token, pos=wordnet_pos))\n",
    "        else:\n",
    "            lemmatized_tokens.append(token)\n",
    "    return lemmatized_tokens\n",
    "\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]\n",
    "\n",
    "def preprocess_text(text, ngram_n=2):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words_en]\n",
    "    tokens = lemmatize_text(tokens)\n",
    "    ngrams = generate_ngrams(tokens, ngram_n)\n",
    "    return tokens + ngrams\n",
    "\n",
    "def process_json(input_file, output_file, ngram_n=2):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for item in data:\n",
    "        processed_item = {}\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, str):\n",
    "                processed_item[key] = preprocess_text(value, ngram_n)\n",
    "            else:\n",
    "                processed_item[key] = value\n",
    "        processed_data.append(processed_item)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "process_json(\"output.json\", \"processed_file.json\", ngram_n=2)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1217784",
   "metadata": {},
   "source": [
    "Ακολουθεί η λίστα με τις βιβλιοθήκες που χρησιμοποιήθηκαν στο πρόγραμμα.\n",
    "Το json χρησιμοποιείται για την αποθήκευση δεδομένων σε αρχείο json.\n",
    "To re χρησιμοποιείται για την εφαρμογή κανονικών εκφράσεων (regex) στην επεξεργασία κειμένου.\n",
    "To nltk είναι η κύρια βιβλιοθήκη που περιέχει υπο-βιβλιοθήκες.\n",
    "Το nltk.tokenize.word_tokenize χρησιμοποιείται για την διαίρεση του κειμένού σε tokens.\n",
    "Το nltk.corpus.stopwords παρέχει τα stopwords.\n",
    "To nltk.stem.WordNetLemmatizer χρησιμοποιείται για την λημματοποίηση των λέξεων.\n",
    "To nltk.pos_tag χρησιμοποιείται για την αναγνώριση του μέρους του λόγου της κάθε λέξης.\n",
    "To nltk.corpus.wordnet χρησιμοποιείται για τον προσδιορισμό της λημματοποιήσης\n",
    "Tα itertools.tee, islice, chain παρέχουν βοηθητικές συναρτήσεις για τον χειρισμό αλληλουχιών.\n",
    "Η εισαγωγή των βιβλιοθηκών γίνεται στο πάνω μέρος του κώδικα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7969f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import tee, islice, chain\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4d91b",
   "metadata": {},
   "source": [
    "Ακολουθεί λήψη των απαραίτητων δεδομένων απο την nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd2c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bc8e1d",
   "metadata": {},
   "source": [
    "Aκολουθεί η συνάρτηση get_wordnet_pos.\n",
    "H συνάρτηση αυτή μετατρέπει τις ετικέτες που παρέχονται από το pos_tag σε μορφή που κατανοέι ο WordNetLemmatizer.\n",
    "Aν η ετικέτα ξεκινάει με J, στο wordnet επιστρέφεται επίθετο. Αντιστοιχα:\n",
    "Αν ξεκινάει με V επιστρέφεται ρήμα.\n",
    "Αν ξεκινάει με N επιστρέφει ουσιαστικό.\n",
    "Αν ξεκινάει με R επιστρέφει επίρρημα.\n",
    "Αν δεν ταιριάζει με κανένα απο τα παραπάνω, παίρνει ως default τιμή του ουσιαστικού"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fabe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef24f6f4",
   "metadata": {},
   "source": [
    "Ακολουθεί η συνάρτηση lemmatize_text.\n",
    "Στην συνάρτηση αυτή γίνεται λημματοποίηση στον κατάλογο λέξεων tokens.\n",
    "Αρχίκα μέσω της pos_tag αναγνωρίζουμε το μέρος του λόγου της κάθε λέξης.\n",
    "Χρησιμοποιούμε την συνάρτηση get_wordnet_pos για να μετατραπεί η λέξη σε μορφή κατανοητή απο τον WordNetLemmatizer.\n",
    "Εάν η λέξη έιναι αλφαβητική (η ταυτοποιήση γίνεται με το if re.match(r'[a-zA-Z]+', token)) γίνεται λημματοποίηση της λέξης. Σε περίπτωση που η \"λεξη\" δεν ειναι αλφαβητική πχ αριθμός, δεν γίνεται επεξεργασία.\n",
    "Τέλος, γίνεται επιστροφή των λημματοποιημένων λέξεων."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1f9e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(tokens):\n",
    "    lemmatized_tokens = []\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    for token, tag in pos_tags:\n",
    "        if re.match(r'[a-zA-Z]+', token):\n",
    "            wordnet_pos = get_wordnet_pos(tag)\n",
    "            lemmatized_tokens.append(lemmatizer_en.lemmatize(token, pos=wordnet_pos))\n",
    "        else:\n",
    "            lemmatized_tokens.append(token)\n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe3874",
   "metadata": {},
   "source": [
    "Ακολουθεί η συνάρτηση generate_ngrams.\n",
    "Στην συνάρτηση αυτή δημιουργούνται n-grams δυο λέξεων από τον κατάλογο tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb1e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(tokens, n=2):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens) - n + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1046b60c",
   "metadata": {},
   "source": [
    "Ακολουθεί η συνάρτηση preprocess_text.\n",
    "Στην συνάρτηση αυτη γίνονται με την σειρά οι εξής λειτουργίες που χρειάονται για την προ-επεξεργασία του κειμένου:\n",
    "Μετατροπη σε πεζά.\n",
    "Αφαίρεση σημείων στίξης.\n",
    "Tokenization.\n",
    "Αφαίρεση των stop words.\n",
    "Λημματοποίηση.\n",
    "N-grams.\n",
    "Τέλος, επιστρέφει τα επεξεργασμένα tokens και n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6969e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, ngram_n=2):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token for token in tokens if token not in stop_words_en]\n",
    "    tokens = lemmatize_text(tokens)\n",
    "    ngrams = generate_ngrams(tokens, ngram_n)\n",
    "    return tokens + ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116e537f",
   "metadata": {},
   "source": [
    "Ακολουθεί η συνάρτηση process_json.\n",
    "Στην συνάρτηση αυτή επεξεργαζόμαστε και αποθηκέυουμε δεδομένα των json αρχείων.\n",
    "Αρχικά, φορτώνεται το αρχείο εισόδου, το output.json με την χρήση της json.load().\n",
    "Γίνεται επεξεργασία του κάθε δεδομένου της αρχείου.\n",
    "Τέλος, γίνεται αποθήκευση των επεξεργασμένων δεδομένων στο output file, το processed_file.json με την χρήση της json.dump().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2854e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_json(input_file, output_file, ngram_n=2):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    processed_data = []\n",
    "    for item in data:\n",
    "        processed_item = {}\n",
    "        for key, value in item.items():\n",
    "            if isinstance(value, str):\n",
    "                processed_item[key] = preprocess_text(value, ngram_n)\n",
    "            else:\n",
    "                processed_item[key] = value\n",
    "        processed_data.append(processed_item)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(processed_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6be76b",
   "metadata": {},
   "source": [
    "Βήμα 3: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70edec6f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
